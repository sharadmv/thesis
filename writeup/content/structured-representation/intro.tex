The goal of Bayesian structured
representation learning (BSRL)
is to combine the
strengths of
Bayesian structured learning
and representation learning,
and ameliorate their weaknesses.
Bayesian structure learning
infers complex latent structure
in data but often suffers from
computational and capacity issues
when dealing with large amounts of
complex, high-dimensional data.
Representational learning methods
like VAEs can model high-dimensional
data flexibly and learn low-dimensional
embeddings of data,
but it's often hard to understand 
and interpret the resulting embeddings,
and guide the representations
with some downstream task in mind.

The strengths and weaknesses of
the methods are complementary
and in this part of the thesis, we demonstrate
explore some key models where we can
achieve the best of both worlds:
compressing complex, high dimensional
data into low-dimensional spaces
where data is organized in a useful way.
Bayesian structure learning
involves incorporating an a priori
idea of how data is organized
into a structure class and
prior distribution.
When combining this idea with representation
learning, we are now inserting
the structure into the embedding space.
The VAE is a logical choice
for representation learning strategy
with BSL in mind, as it includes
a prior distribution over the latent space
and this prior distribution
offers an opportunity to specify
how data is organized.

In BSRL, we put a Bayesian structured prior
on the latent space of the codes $\latentdataset$
of a VAE,
a process described by the following model:
\begin{align*}
    \structure &\sim \p(\structure) \\
    \latentdataset \given \structure &\sim \p(\latentdataset \given \structure) \\
    \x_\n \given \z_\n &\sim \p_\genparam(\x_\n \given \z_\n)
\end{align*}
This process is just the concatenation
of the two generative process we have discussed earlier,
except the structure influences
the latent space rather than the data directly.
This graphical model is pictured in
\autoref{fig:bsrl}.

\begin{figure}[H]
    \centering
    \includegraphics[]{tikz/loracs/ltmc}
    \caption{The graphical model for
    Bayesian structured representation learning}
    \label{fig:bsrl}
\end{figure}

The design space of BSRL problems
is that of BSL, namely
picking a structure class
and prior distribution
but with the added ability
to model data
with complicated likelihoods.
The encoder and decoder architectures
in the VAE are important
choices when performing inference,
but in this thesis, we focus on
the choice of prior distribution.
The hope is to pick a structured
prior that influences the representations
learned by a VAE.
This strategy may appear to
achieve the best of both worlds,
but it comes
with its own challenges.
One challenge is that inference in BSRL models
can be far challenging
more than in isolated BSL or representation
learning models. Consider Bayesian
nonparametric hierarchical clustering,
which uses MCMC, and the VAE,
which uses gradient-based variational
inference. The combined model
is easy to write down and understand,
but it is not immediately clear
what an inference procedure that combines
MCMC and variational inference
will look like. Another challenge 
is that of scaling structure learning
algorithms to large datasets. Although
representation learning methods help
with the problem of high-dimensional
complex data, structure learning methods
like Bayesian nonparametric hierarchical clustering
will struggle with large amounts of data.
Finally, what can we do with a BSRL model
that couldn't be done with a regular BSL or representation
learning model?

The remainder of this thesis
explores these challenges.
The next chapter deals with algorithmic
hurdles that come with Bayesian
inference in BSRL. Specifically,
we present neural variational message
passing (NVMP), an extension of \citet{Johnson2016}
that performs non-conjugate variational inference 
using recognition networks.
The following chapter introduces
the LORACs prior, which
bridges the gap between
Bayesian nonparametric
hierarchical clustering and the VAE.
We address the challenge of scaling
Bayesian nonparametrics to large datasets
and also explore how BNHC can help improve
the quality of representations learned
by a VAE.
The final chapter details a novel
contribution in the space
of model-based reinforcement learning, where
the task is controlling a robot from raw images.
The contribution is SOLAR, an algorithm
that applies linear control
in the latent space of a BSRL model,
which we apply successfully to real robots.

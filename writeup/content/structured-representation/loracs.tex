Variational autoencoders \citep[VAEs; ][]{kingma2013autoencoding,rezende2014stochastic} are a popular class of deep latent-variable models. The VAE assumes that observations $x$ are generated by first sampling a latent vector $z$ from some tractable prior $p(z)$, and then sampling $x$ from some tractable distribution $p(x\mid g_\theta(z))$.
For example, $g_\theta(z)$ could be a neural network with weights $\theta$ and $p(x\mid g_\theta(z))$ might be a Gaussian with mean $g_\theta(z)$.

VAEs, like other unsupervised latent-variable models \citep[e.g.; ][]{tipping1999probabilistic,blei2003latent}, can uncover latent structure in datasets.
In particular, one might hope that high-level characteristics of the data are encoded more directly in the geometry of the latent space $z$ than they are in the data space $x$. For example, when modeling faces one might hope that one latent dimension corresponds to pose, another to hair length, another to gender, etc. 

What kind of latent structure will the VAE actually discover?
\citet{hoffman2016elbo} observe that the ELBO encourages the model to make the statistics of the population of encoded $z$ vectors resemble those of the prior, so that $p(z)\approx\mathbb{E}_\mathrm{population}[p(z\mid x)]$.
The prior $p(z)$ therefore plays an important role in shaping the geometry of the latent space.
For example, if we use the ``default'' prior $p(z)=\N(z; 0, I)$, then we are asking the model to explain the data in terms of smoothly varying, completely independent factors \citep{burgess2018understanding}. These constraints may sometimes be reasonable---for example, geometric factors such as pose or lighting angle may be nearly independent and rotationally symmetric. But some natural factors exhibit dependence structure (for example, facial hair length and gender are strongly correlated), and others may have nonsmooth structure (for example, handwritten characters naturally cluster into discrete groups).

In this paper, we propose using a more opinionated prior on the VAE's latent vectors: the time-marginalized coalescent \citep[TMC; ][]{boyles2012time}.
The TMC is a powerful, interpretable Bayesian nonparametric hierarchical clustering model that can encode rich discrete and continuous structure.
Combining the TMC with the VAE combines the strengths of Bayesian nonparametrics (interpretable, discrete structure learning) and deep generative modeling (freedom from restrictive distributional assumptions).


% We can consider the models from an information-theoretic perspective \citep{alemi18brokenelbo}:
% a classical autoencoder will try to make $z$ encode as much information about $x$ as possible, subject to the constraints imposed by the training procedure and function class, while
% the evidence lower bound (ELBO) used to train VAEs adds a regularization term that penalizes the model for encoding too much information in $z$.
% But this says nothing about the geometry of the latent space, which is critical for downstream applications. For example, if we want to use the inferred $z$ vectors for a classification task we may want them to organize into well-separated clusters.


% If there are multiple factors of variation in the data, we claim that the model will choose to model those whose statistics

% For example, \citet{burgess2018understanding} observe that the common normal prior $p(z) = \N(z; 0, I)$ encourages the model to learn \emph{disentangled} representations---this 


% This implies that the prior $p(z)$ plays an important role in determining what kind of latent structure the VAE discovers.


% Approximate maximum-likelihood training of VAEs can encourage the latent vector $z$ to have high mutual information with the observations $x$ \citep{alemi18brokenelbo}, while encouraging the model to make the statistics of the encoded $z$ vectors resemble those of the prior so that $p(z)\approx\mathbb{E}_x[p(z\mid x)]$ \citep{hoffman2016elbo}.


% If all goes well, the posterior $p(z\mid x)$ will encode useful information about $x$.

% find an encoding scheme such that the aggregate posterior $\tilde p(z)\triangleq \frac{1}{N}\sum_n $

% encode information about $x$ in $z$ in a way that conforms to the (usually simple) 

% %One advantage of VAEs over other powerful deep density estimators based on autoregressive models \citep[e.g.; ][]{salimans2017pixelcnn++,oord2016wavenet,papamakarios2017maf} is that the latent vectors $z$ may encode 

% - Representation learning with VAEs, latent space encodes semantics

% - Often times people use generic marginals (N(0, I) beta VAE, (factors may not be independent hair length, beard length)

% - Alternatively use a powerful prior (MAF), but encode smoothness assumptions but aren't appropriate if data has natural latent discrete structure

Our contributions are:
\begin{itemize}
    \item We propose a deep Bayesian nonparametric model that can discover hierarchical cluster structure in complex, high-dimensional datasets.
    \item We develop a minibatch-friendly inference procedure for fitting TMCs based on an inducing-point approximation, which scales to arbitrarily large datasets.
    \item We show that our model's learned latent representations consistently outperform those learned by other variational (and classical) autoencoders when evaluated on downstream classification and retrieval tasks.
\end{itemize}

\section{Background}

\subsection{Bayesian priors for hierarchical clustering}
\label{sec:bnhc}

\begin{figure*}[t]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[frame, width=\textwidth]{img/loracs/tmc/tmc-small-cropped}
\end{subfigure}
~
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \reflectbox{\includegraphics[frame, width=\textwidth]{img/loracs/tmc/tmc-big-1-cropped}}
\end{subfigure}
\caption{Independent samples from a time-marginalized coalescent (TMC) prior and two-dimensional Gaussian random walk likelihood model (10 and 300 leaves respectively). 
Contours in the plots correspond to posterior predictive density $r(z_{N + 1} \given z_{1:N}, \tau)$.
As the number of leaves grow, the predictive density grows more complex.}
\label{fig:tmc-samples}
\end{figure*}

Hierarchical clustering is a flexible
tool in exploratory data analysis
as trees offer visual, interpretable
summaries of data. Typically,
algorithms for hierarchical clustering are
either agglomerative
(where data are recursively, greedily merged to form
a tree from the bottom-up)
or divisive (where data
are recursively partitioned, forming a tree from the
top-down). Bayesian nonparametric hierarchical clustering (BNHC) 
additionally incorporates uncertainty over tree
structure by introducing
a prior distribution over trees $r(\tau)$ and
a likelihood model for data $r(z_{1:N} \given \tau)$,
with the goal of sampling
the posterior distribution $r(\tau \given z_{1:N})$.\footnote{We use $r$ to denote probability distributions
relating to the TMC and distinguish from $p$ and $q$
distributions used later in the paper.}

In this paper, we focus on
rooted binary trees with $N$ labeled leaves 
adorned with branch lengths,
called \emph{phylogenies}.
Prior distributions over phylogenies
often take the form of a stochastic generative
process in which a tree is built
with random merges, as in the Kingman coalescent \citep{kingman1982coalescent},
or random splits, as in the Dirichlet diffusion tree \citep{neal2003density}.
These nonparametric distributions have
helpful properties, such as exchangeability,
which enable efficient Bayesian inference.
In this paper, we focus on the time-marginalized
coalescent \citep[TMC; ][]{boyles2012time}, which decouples the
distribution over tree structure and branch length,
a property that helps simplify inference down the line.

\subsubsection{Time-marginalized coalescent (TMC)}
The time-marginalized coalescent defines a prior distribution over phylogenies.
A phylogeny $\tau = (V, E, T)$ is a directed rooted full binary tree, with vertex set $V$ and edges $E$, together with time labels $T: V \to [0, \, 1]$ where we denote $t_v = T(v)$.
The vertex set $V$ is partitioned into $N$ leaf vertices $V_\text{leaf}$ and $N-1$ internal vertices $V_\text{int}$, so that $V = V_\text{int} \cup V_\text{leaf}$, and we take $V_\text{leaf} = \{1, 2, \ldots, N\}$ to simplify notation for identifying leaves with $N$ data points.
The directed edges of the tree are encoded in the edge set $E \subset V_\text{int} \times V$, where we denote the root vertex as $v_\text{root}$ and for $v \in V \setminus \{v_\text{root}\}$ we denote the parent of $v$ as $\pi(v) = w$ where $(w, v) \in E$.

The TMC samples a random tree structure $(V, E)$ by a stochastic process in which the $N$ leaves are recursively merged uniformly at random until only one vertex is left.
This process yields the probability mass function on valid $(V, E)$ pairs given by
\begin{equation}
    r(V, E) = \frac{(N - 1)!}{\prod_{v \in V_\text{int}} c(v)} \prod_{i = 1}^{N-1} {i+1 \choose 2}^{-1},
\end{equation}
where $c(v)$ denotes the number of internal vertices in the subtree rooted at $v$.
Given the tree structure, time labels are generated via the stick-breaking process
\begin{equation}
    t_v = \begin{cases} 0 & v = v_\text{root}, \\ 1 & v \in V_\text{leaf}, \\ t_{\pi(v)} - \beta_v (1 - t_{\pi(v)}) & v \in V_\text{int} \setminus \{v_\text{root}\}, \end{cases}
\end{equation}
where $\beta_v \iid \sim \mathrm{Beta}(a, b)$ for $v \in V$. These time labels encode a branch length $t_v - t_{\pi(v)}$ for each edge $e = (\pi(v), v) \in E$. We denote the overall density on phylogenies with $N$ leaves as $\mathrm{TMC}_N(\tau; a, b)$.

Finally, to connect the TMC prior to data in $\mathbb{R}^d$, we define a likelihood model $r(z_{1:N} \given \tau)$ on $N$ data points, with $z_n$ corresponding to the leaf vertex $n \in V_\text{leaf}$.
We use a Gaussian random walk (GRW), where for each vertex $v \in V$ a location $z_v \given z_{\pi(v)}$ is sampled according to a Gaussian distribution centered at its parent's location with variance equal to the branch length,
\begin{equation}
    z_v \given z_{\pi(v)} \sim \N(z_{\pi(v)}, (t_v - t_{\pi(v)})I), \quad v \in V \setminus \{v_\text{root}\},
    \notag
\end{equation}
and we take $z_{v_\text{root}} \sim \N(0, I)$.
As a result of this choice, we can exploit the Gaussian graphical model structure to efficiently marginalize out the internal locations $z_v$ associated with internal vertices $v \in V_\text{int}$ and evaluate the resulting marginal density $r(z_{1:N} \given \tau)$. For details
about this marginalization, please refer to \autoref{sec:algorithm-details}.
The final overall density is written as
\begin{equation}
    r(z_{1:N}, \tau) = \mathrm{TMC}_N(\tau; a, b)r(z_{1:N} \given \tau).
\end{equation}
For further details and derivations related to the TMC,
please refer to \citet{boyles2012time}.

\subsubsection{TMC posterior predictive density}
The TMC with $N$ leaves and a GRW likelihood model can be a prior on a set of $N$ hierarchically-structured data, i.e. data that correspond to nodes with small tree distance should have similar location values.
In addition, it also acts as a density from which we can sample new data.
The posterior predictive density $r(z_{N + 1} \given z_{1:N}, \tau)$ is easy to sample thanks to the exchangeability of the TMC.

To sample a new data point $z_{N + 1}$, we select a branch (edge) and a time to attach a new leaf node.
The probability $r(e_{N+1} \given V, E)$ of selecting branch $e_{N+1}$ is proportional to the probability under the TMC prior of the tree with a new leaf attached to branch $e_{N+1}$.
The density $r(t_{N+1} \given e_{N+1}, V, E)$ for a time label $t_{N+1}$ is determined by the stick-breaking process (see \autoref{sec:algorithm-details} for details).
% given by a shifted-and-scaled $\mathrm{Beta}(a, b)$ distribution and inserting it into the stick-breaking process, creating a new time $t_{N+1}$ for the node attached to branch $b_{N+1}$.
Both of these probabilities are easy to calculate and sample due to the exchangeability of the TMC.

The new location $z_{N + 1}$ can be sampled from  $r(z_{N + 1} \given e_{N + 1}, t_{N + 1}, \tau)$, which is
the Gaussian distribution that comes out of the GRW likelihood model.
Pictured in \autoref{fig:tmc-samples} are samples from a TMC prior and GRW likelihood, where contours correspond to $r(z_{N + 1} \given z_{1:N}, \tau)$.
In addition to modeling hierarchical structure, the TMC is a flexible nonparametric density estimator.

\subsubsection{TMC inference}

The posterior distribution $r(\tau \given z_{1:N})$
is analytically intractable due to the normalization constant
$r(z_{1:N})$ involving a sum over all tree structures,
but it can be approximately sampled via Markov chain Monte-Carlo (MCMC) methods.
% In MCMC, we construct a proposal distribution
% $T(\tau' \given \tau)$
% a Markov chain with transition dynamics $T$
% has joint distribution $r(\tau, z_{1:N})$.
We utilize the Metropolis-Hastings
algorithm with a subtree-prune-and-regraft (SPR)
proposal distribution \citep{neal2003density}. An SPR proposal
picks a subtree uniformly at random from $\tau$
and detaches it.
It is then attached back on the tree
to a branch and time picked uniformly at random.
% The new tree $\tau'$ is accepted as the next
% state in the Markov chain
% with probability $\min\left(1, \frac{r(\tau', z_{1:N})T(\tau \given \tau')}{r(\tau, z_{1:N})T(\tau' \given \tau)}\right)$.
The Metropolis-Hastings acceptance probability is efficient to compute because the joint density $r(\tau, z_{1:N})$ can be evaluated using belief propagation to marginalize the latent values at internal nodes of $\tau$, and many of the messages can be cached.
See \autoref{sec:algorithm-details} for details.

\subsection{Variational autoencoder}
The variational autoencoder (VAE) is a generative model
for a dataset $x_{1:N}$
wherein latent vectors $z_{1:N}$ are sampled
from a prior distribution
and then individually passed into
a neural network observation model with parameters $\theta$,
\begin{equation}
    \begin{split}
        z_{1:N} \sim p(z_{1:N}),
        \qquad
        x_n \given z_n \sim p_\theta(x_n \given z_n),
    \end{split}
\end{equation}
We are interested in the posterior distribution
$p(z_n \given x_n)$, which is not analytically tractable
but can be approximated with a variational distribution
$q_\phi(z_n \given x_n)$, typically a neural network
that outputs parameters of a Gaussian distribution.
The weights of the approximate posterior can be learned
by optimizing the evidence-lower bound (ELBO),
\begin{equation}
    \begin{split}
    \L[q] &\triangleq \E_q \left[\log \frac{p_\theta(x_{1:N}, z_{1:N})}{\prod_n q_\phi(z_n \given x_n)}\right] \\
          %&= \left(\sum_{n = 1}^N \E_q\left[\log p(x_n \given z_n)\right]\right) - \KL{q_\phi(z_{1:N} \given x_{1:N})}{p(z_{1:N})}.
    \end{split}
\end{equation}
The parameters of the model, $\theta$ and $\phi$,
are learned via stochastic gradient ascent on
the ELBO, using the reparametrization trick
for lower variance gradients \citep{kingma2013autoencoding, rezende2014stochastic}.

\section{The TMC-VAE}
\label{sec:tmc-vae}

The choice of prior distribution in the VAE significantly affects the autoencoder and resulting latent space.
The default standard normal prior, which takes $z_n \iid\sim \N(0, I)$, acts as a regularizer on an otherwise unconstrained autoencoder, but can be restrictive and result in overpruning \citep{burda2015importance}.
Extremely flexible, learnable distributions like masked autoregressive flow (MAF) priors \citep{papamakarios2017masked} enable very rich latent spaces, but don't encode any interpretable bias for organizing the latent space (except perhaps smoothness).

In this paper, we explore the TMC prior for the VAE, which could potentially strike a sweet spot between restrictive and flexible priors.
We generate the latent values $z_{1:N}$ of a VAE according to the TMC prior, then generate observations $x_{1:N}$ using a neural network observation model,
\begin{gather}
        \tau \sim \mathrm{TMC}_N(\tau; a, b), \\
        z_{1:N} \given \tau \sim r(z_{1:N} \given \tau),
        \qquad
        x_n \given z_n \sim p_\theta(x_n \given z_n).
\end{gather}
The TMC-VAE is a coherent
generative process that captures
discrete, interpretable structure in the latent space.
A phylogeny not only has an intuitive inductive bias,
but can be useful for
exploratory data analysis
and introspecting the latent space itself.

Consider doing inference in this model:
first assume variational distributions 
$q_\phi(z_n \given x_n)$ (as in the VAE)
and $q(\tau)$, which results in the
ELBO,
\begin{equation}
    \begin{split}
    \L[q] &= \E_q\left[\log \frac{p(\tau, z_{1:N}, x_{1:N})}{q(\tau)\prod_n q(z_n \given x_n)}\right] \\
    \end{split}
\end{equation}
For fixed $q_\phi(z_n \given x_n)$, we can
sample the optimal $q^*(\tau)$,
\begin{equation}
    q^*(\tau) \propto \exp\{\E_q\left[\log p(\tau, z_{1:N}, x_{1:N})\right]\}
\end{equation}
Because $p(z_{1:N} \given \tau)$ is jointly Gaussian (factorizing
according to tree structure) and $q_\phi(z_n \given x_n)$ is Gaussian, 
expectations with respect to $z_{1:N}$
can move into $\log p(\tau, z_{1:N}, x_{1:N})$. This
enables sampling the expected joint likelihood $\E_q\left[\log p(\tau, z_{1:N})\right]$
using SPR Metropolis-Hastings.
However, optimizing this ELBO is problematic.
$p(z_{1:N} | \tau)$
does not factorize independently, so computing
unbiased gradient estimates from minibatches is impossible
and requires evaluating all the data.
Furthermore, the TMC is limiting
from a computational perspective.
Since a phylogeny has as many leaves
as points in the dataset, 
belief propagation over internal nodes
of the tree slows down linearly as 
the size of the dataset grows.
In addition,
% the search space of trees
% grows exponentially with the size of the dataset,
% rendering MCMC unusable with extremely
SPR proposals mix very slowly for large trees.
We found these limitations 
make the model impractical for datasets
of more than 1000 examples.

In the next section, we address these computational
issues, while retaining
the interesting properties of the TMC-VAE.

\begin{figure*}[t]
\centering
\begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics{tikz/loracs/ltmc.tikz}
    \caption{TMC-VAE graphical model}
\end{subfigure}
\begin{subfigure}[t]{0.34\textwidth}
    \centering
    \includegraphics{tikz/loracs/iltmc.tikz}
    \caption{\acronym-VAE graphical model}
\end{subfigure}
\begin{subfigure}[t]{0.34\textwidth}
    \centering
    \includegraphics{tikz/loracs/iltmc-v.tikz}
    \caption{\acronym-VAE variational factors}
\end{subfigure}
\caption{Graphical models and variational
approximations for TMC models described in the paper}
\label{fig:graphical-models}
\end{figure*}

\section{\acronym\;prior for VAEs}

In this section, we introduce
a novel approximation to the TMC
prior, which preserves many 
desirable properties
like structure and interpretability,
while being computationally viable.
Our key idea is to use a set of
learned \emph{inducing points}
as the leaves of the tree in the latent
space, analogous to inducing-input
approximations for Gaussian processes \citep{snelson2006sparse}. 
In this model, latent vectors $z_{1:N}$
are not directly hierarchically clustered,
but are rather independent samples
from the induced posterior predictive density
of a TMC.
We call this the \acronymexplanation\;(\emph{\acronym}, pronounced ``\acronympronunciation'') prior.

To define the \acronym\;prior $p(\tau, z_{1:N})$, we first define an auxiliary TMC distribution $r(\tau, s_{1:M})$ with $M$ leaf locations $s_{1:M}$.
We treat $s_{1:M}$ as a set of learnable free parameters, and define the conditional $r(\tau \given s_{1:M})$ as the \acronym\;prior on phylogenies $\tau$:
\begin{equation}
%   \tau \sim p(\tau ; s_{1:M}) \triangleq r(\tau \given s_{1:M}).
  p(\tau ; s_{1:M}) \triangleq r(\tau \given s_{1:M}).
\end{equation}
That is, we choose the prior on phylogenies $\tau$ to be the posterior distribution of a TMC with pseudo-observations $s_{1:M}$.
Next, we define the \acronym\;prior on locations $z_n\mid \tau$ as a conditionally independent draw from the predictive distribution $r(s_{M+1} \given \tau, s_{1:M})$, writing the sampled attachment branch and time as $e_n$ and $t_n$, respectively:
\begin{gather}
    % b_n, t_n \given \tau \sim p(b_n, t_n \given \tau)
    p(e_n, t_n \given \tau)
    \triangleq r(e_{M+1} = e_n, t_{M+1} = t_n \given \tau),
    \notag
    \\
    % z_n \given b_n, t_n \tau \sim p(z_n \given b_n, t_n, \tau)
    p(z_n \given e_n, t_n, \tau)
    \triangleq r(s_{M+1} = z_n \given e_n, t_n, \tau, s_{1:M}).
\end{gather}
To complete the model, we use an observation likelihood parameterized by a neural network, writing
\begin{equation}
    x_n \given z_n \sim p_\theta(x_n \given z_n).
\end{equation}
%
% We introduce a set of 
% $M$ learnable inducing points $s_{1:M}$
% that are leaves of a phylogeny $\tau$,
% and adopt the following model:
%
% \todo{simplify this model description}
% \begin{equation}
%     \begin{split}
%         \tau &\sim p(\tau ; s_{1:M}) \triangleq r(\tau \given s_{1:M}) \\
%         z_n \given \tau &\sim p(z_n \given \tau; s_{1:M}) \triangleq r(s_{M + 1} = z_n \given \tau, s_{1:M}) \\
%         x_n \given z_n &\sim p_\theta(x_n \given z_n)
%     \end{split}
% \end{equation}
% We choose our prior
% on phylogenies to be the posterior TMC
% with leaves $s_{1:M}$.
% The prior on latent vectors $p(z_n \given \tau; s_{1:M})$
% is the posterior predictive density for $\tau$ and $s_{1:M}$
% and the observation model is parametrized by a neural network,
% as in the VAE. Effectively, we have put a flexible,
% nonparametric density estimator as our VAE prior
% with the added benefit of learned discrete structure.
%
%By limiting the size of the tree to $M$ leaves,
%we've circumvented some computational issues
%issues that come with larger trees.
%This comes at the cost of the
%expressiveness and interpretability
%of this prior distribution, but
%this can be tuned by making $M$
%larger. However, some additional
%approximations will need to be made for tractable inference.
%
% For the purpose of inference, 
% we work with a more explicit version
% of the model, where the branch and time of the
% posterior predictive distribution are
% are sampled explicitly.
% \todo{use this model for induced tmc}
% \begin{equation}
%     \begin{split}
%         \tau &\sim p(\tau ; s_{1:M}) \\
%         b_n, t_n &\sim p(b_n, t_n \given \tau) \\
%         z_n \given b_n, t_n, \tau &\sim p(z_n \given b_n, t_n, \tau; s_{1:M}) \triangleq r(s_{M + 1} = z_n \given b_n, t_n, \tau) \\
%         x_n \given z_n &\sim p_\theta(x_n \given z_n)
%     \end{split}
% \end{equation}
%
By using the learned inducing points $s_{1:M}$, we avoid the main difficulty of inference in the TMC-VAE of Section~\ref{sec:tmc-vae}, namely the need to do inference over all $N$ points in the dataset.
Instead, dependence between datapoints is mediated by the set of inducing points $s_{1:M}$, which has a size independent of $N$.
As a result, with the \acronym\;prior, minibatch-based learning becomes tractable even for very large datasets.
The quality of the approximation to the TMC-VAE can be tuned by adjusting the size of $M$.

However, this technique presents its own inference challenges.
Sampling the optimal variational factor $q^*(\tau)$
is no longer an option as it was in the TMC-VAE:
\begin{equation}
\begin{split}
    q^*(\tau; s_{1:M}) 
    &\textstyle\propto \exp{\E_q\left[\log p(\tau, z_{1:N}, x_{1:N})\right]} \\
    &\textstyle\propto \exp{\log p(\tau ; s_{1:M}) + \sum_n \E_q\left[p(z_n \given e_n, t_n, \tau)\right]} \\
    &\textstyle\propto \exp{\log \mathrm{TMC}_N(\tau; a, b)
    \\ &\textstyle\qquad\quad + \sum_{m = 1}^M \log r(s_m \given s_{1:m - 1}, \tau)
    \\ &\textstyle\qquad\quad + \sum_n \E_q\left[\log p(z_n \given e_n, t_n, \tau)\right]}.
\end{split}
\end{equation}
This term has a sum over $N$ expectations; therefore computing this likelihood
for the purpose of MCMC
would involve passing the entire dataset through a neural network.
Furthermore, the normalizer for this likelihood is intractable,
but necessary for computing gradients w.r.t $s_{1:M}$.
We therefore avoid using the optimal $q^*(\tau; s_{1:M})$
and set
$q(\tau; s_{1:M})$ to the prior.
This has the additional computational advantage of cancelling out
the $\mathbb{E}_q[\log p(\tau)]$ term in the ELBO, which also has an intractable normalizing constant.
If the inducing points are chosen so that they contain most of the information about the hierarchical organization of the dataset, then the approximation $p(\tau \given z)\approx r(\tau \given s_{1:M})=p(\tau)$ will be reasonable.
% Although this choice adds some approximation error, we still fit a distribution over tree structures by optimizing the inducing points $s_{1:M}$ which parameterize the prior.

We also fit the variational factors
$q(e_n)$,
$q_{\xi}(t_n \given e_n, z_n; s_{1:M})$,
and
$q_\phi(z_n \given x_n)$.
The factor for attachment times,
$q_{\xi}(t_n \given e_n, z_n; s_{1:M})$, is a
recognition network that outputs
a posterior over attachment times for a particular branch.
Since the $q(\tau; s_{1:M})$ and $p(\tau; s_{1:M})$ terms
cancel out, we obtain the following ELBO (some notation suppressed for simplicity):
\begin{equation}
    \begin{split}
    \L[q] &\triangleq \E_q \left[\log \frac{\prod_n p(e_n, t_n \given \tau) p(z_n \given e_n, t_n, \tau)p(x_n \given z_n)}{\prod_n q(e_n)q(t_n \given e_n, z_n)q(z_n \given x_n)}\right].
    \end{split}
\end{equation}
This ELBO can be optimized by first computing
\begin{equation}
    q^*(e_n) = \exp{\E_q\left[\log p(e_n \given t_n, z_n, \tau; s_{1:M})\right]}
\end{equation}
and computing gradients with respect to $\theta$, $s_{1:M}$,
$\phi$, and $\xi$ using a Monte-Carlo estimate of the ELBO
using samples from $q(\tau; s_{1:M})$, $q^*(e_n)$, $q_\phi(z_n \given x_n)$, and $q_{\xi}(t_n \given e_n, z_n; s_{1:M})$.
The factor $q(\tau; s_{1:M})$ can
be sampled using vanilla SPR Metropolis-Hastings. 
The detailed inference procedure can be found in \autoref{sec:algorithm-details}.

\section{Related work}
As mentioned above, \acronym\;connects various ideas in the literature, including Bayesian nonparametrics \citep{boyles2012time}, inducing-point approximations \citep[e.g.; ][]{snelson2006sparse,tomczak2018vae},
and amortized inference \citep{kingma2013autoencoding,rezende2014stochastic}.

Also relevant is a recent thread of efforts to endow VAEs with the interpretability of graphical models \citep[e.g.; ][]{johnson2016svae,lin2018variational}.
In this vein, \citet{goyal2017nonparametric} propose using a different Bayesian nonparametric tree prior, the nested Chinese restaurant process (CRP) \citep{griffiths2004hierarchical}, in a VAE.
We chose to base \acronym\;on the TMC instead, as the posterior predictive distribution of an nCRP is a finite mixture, whereas the TMC's posterior predictive distribution has more complex continuous structure.
Another distinction is that \citet{goyal2017nonparametric} only consider learning from pretrained image features, whereas our approach is completely unsupervised.

\section{Results}
In this section, we 
analyze properties of the
\acronym\;prior,
focusing on
qualitative aspects, like exploratory data analysis and interpretability,
and quantitative aspects, like few-shot classification and information retrieval.
\paragraph{Experimental setup} We evaluated
the \acronym\;prior on three separate
datasets: dynamically binarized MNIST \citep{MNIST}, Omniglot \citep{omniglot},
and CelebA \citep{celeba}. 
For all three experiments,
we utilized convolutional/deconvolutional encoders/decoders
and a 40-dimensional
latent space (detailed architectures can be found
in \autoref{sec:implementation-details}).
We used 200, 1000, and 500 inducing points for MNIST, Omniglot, and CelebA respectively 
with TMC parameters $a = b = 2$.
$q_{\xi}(t_n \given e_n, z_n; s_{1:M})$ was a
two-layer 500-wide neural network
with ReLU activations that output parameters of a
logistic-normal distribution over stick size
and all parameters were optimized with Adam \citep{kingma2015adam}.
Other implementation details can be found in \autoref{sec:implementation-details}.

\subsection{Qualitative results}

A hierarchical clustering in the latent space
offers a unique opportunity for interpretability
and exploratory data analysis,
especially when the data are images.
Here are some methods for users to obtain
useful data summaries and explore a dataset.

\paragraph{Visualizing inducing points}
We first inspect the learned inducing points $s_{1:M}$
by passing them through the decoder.
Visualized in \autoref{fig:mnist-inducing}
are the 200 learned inducing points for MNIST.
The inducing points are all unique
and are cleaner than pseudo-input reconstructions from VampPrior (shown in
\autoref{fig:mnist-vamp-inducing-outputs}).
Inducing points can help summarize a
dataset, as visualizations of the latent space
indicate they spread out and cover the data
(see \autoref{fig:mnist-tsne-tree}). Inducing points
are also visually unique and sensible
in Omniglot and CelebA (see
\autoref{fig:omniglot-inducing-points} and \ref{fig:celeba-inducing-points}).

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/loracs/mnist/mnist-inducing-points.png}
\caption{Learned inducing points for a \acronym(200) prior on MNIST.}
\label{fig:mnist-inducing}
\end{figure}

\paragraph{Hierarchical clustering}
We can sample
$q(\tau ; s_{1:M})$ to obtain
phylogenies
over the inducing points,
and can visualize these clusterings
using the decoded inducing points;
subtrees from a sample in each dataset
are visualized in \autoref{fig:subtrees}.
In MNIST, we find large subtrees
correspond to the discrete classes
in the dataset. In Omniglot,
subtrees sometimes correspond to language groups
and letter shapes. In CelebA,
we find subtrees sometimes correspond
to pose or hair color and style.

We can further use the time at each internal node
to summarize the data at many levels of granularity.
Consider ``slicing'' the hierarchy
at a particular time $t$ by
taking every branch $(\pi(v), v) \in E$ with $t_{\pi(v)} \leq t < t_v$
and computing the corresponding expected Gaussian random walk value at time $t$.
At times closer to zero, we slice
fewer branches and are closer to the root
of the hierarchy, so the value at the slice
looks more like the mean of the data.
In \autoref{fig:celeba-evolution}, 
we visualize this process over a subset
of the inducing points of CelebA.
Visualizing the dataset in this way
reveals cluster structure at 
different granularities
and offers an evolutionary interpretation
to the data, as leaves that coalesce 
more ``recently'' are likely to be closer in 
the latent space.

Although the hierarchical clustering is only
over inducing points, we can still visualize
where real data belong on the hierarchy
by computing $q^*(e_n)$ and
attaching the data to the tree.
By doing this for many points of data, and removing
the inducing points from the tree,
we obtain an induced hierarchical clustering.
%We visualize some example induced hierarchical clusterings
%in \autoref{fig:induced-clusters}.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.2\textwidth}
\centering
\includegraphics[width=\textwidth]{img/loracs/mnist/subtree1.png}
\caption{MNIST}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
\centering
\includegraphics[width=\textwidth]{img/loracs/omniglot/subtree1.png}
\caption{Omniglot}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
\centering
\includegraphics[width=\textwidth]{img/loracs/celeba/subtree1.png}
\caption{CelebA}
\end{subfigure}
\caption{An example learned subtree from a sample of $q(\tau; s_{1:M})$ for each dataset. 
Leaves are visualized by passing inducing points throught the decoder.}
\label{fig:subtrees}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/loracs/celeba/celeba-evolution.png}
\caption{The evolution of a CelebA over a subset of inducing points. We create
this visualization by taking slices of the tree at particular times
and looking at the latent distribution 
at each of the sliced branches.}
\label{fig:celeba-evolution}
\end{figure}

\paragraph{Generating samples} 
Having fit a generative model to our data,
we can visualize samples from the model.
Although we do not expect the samples
to have fidelity and sharpness comparable
to those from GANs or state-of-the-art
decoding networks \citep{radford2015unsupervised, salimans2017pixelcnn++},
sampling with the
\acronym\;prior can help us understand the latent space.
To draw a sample from a TMC's posterior predictive density,
we first sample a branch and time, assigning
the sample a place in the tree.
This provides each generated sample a \emph{context},
i.e., the branch and subtree it was generated from.
However, learning a \acronym\;prior allows us to
conditionally sample in a novel way. By restricting
samples to a subtree, we can generate samples
from the support of the posterior predictive density
limited to that subtree. This enables
conditional sampling at many levels of
the hierarchy. We visualize examples
of this in \autoref{fig:subtree-samples}
and \autoref{fig:celeba-samples}.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{img/loracs/mnist/mnist-samples.png}
\caption{MNIST}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{img/loracs/omniglot/omniglot-samples.png}
\caption{Omniglot}
\end{subfigure}
\caption{Conditional samples from subtrees.}
\label{fig:subtree-samples}
\end{figure}


\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[frame, width=\textwidth]{img/loracs/celeba/celeba-samples-1.png}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[frame, width=\textwidth]{img/loracs/celeba/celeba-samples-2.png}
\end{subfigure}
\caption{Samples from subtrees of CelebA.}
\label{fig:celeba-samples}
\end{figure}

\subsection{Quantitative results}

We ran experiments designed to
evaluate the usefulness of the \acronym's
learned latent space for downstream tasks. We 
compare the \acronym\;prior
against a set of baseline priors
on three different tasks:
few-shot classification,
information retrieval,
and generative modeling.
Our datasets are
dynamically binarized MNIST and Omniglot 
(split by instance) and
our baselines are representations
learned with the same encoder-decoder
architecture and latent dimensionality\footnote{Following the defaults
in the author's reference implementation, we evaluated
DVAE\# on statically binarized MNIST with smaller neural networks, but
with a higher-dimensional latent space.} but substituting
the following prior distributions over $z$:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item No prior
    \item Standard normal prior
    \item VampPrior \citep{tomczak2018vae} - 500 pseudo-inputs for MNIST, 1000 for Omniglot
    \item DVAE$\sharp$ \citep{vahdat2018dvae} - latent vectors are 400-dimensional, formed from concatenating binary latents, encoder and decoder are two-layer feed-forward networks with ReLU nonlinearities
    \item Masked autoregressive flow \citep[MAF; ][]{papamakarios2017masked} - two layer, 512 wide MADE
\end{itemize}
\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.35\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/loracs/mnist/cf-mnist.png}
    \caption{MNIST}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/loracs/omniglot/cf-omniglot.png}
    \caption{Omniglot}
\end{subfigure}
\caption{Few-shot classification results}
\vspace{-0.5cm}
\label{fig:fewshot}
\end{figure}
\paragraph{Few-shot classification}
In this task,
we train a
classifier with
varying numbers of labels and measure
test accuracy. We pick
equal numbers of labels per class
to avoid imbalance and we use
a logistic regression classifier
trained to convergence to avoid
adding unnecessary degrees of freedom to the experiment.
We replicated the experiment across
20 randomly chosen label sets for MNIST
and 5 for Omniglot.
The test accuracy on
these datasets is visualized in 
\autoref{fig:fewshot}. For MNIST, we 
also manually labeled inducing points and
found that training a classifier on 200 and 500 inducing points
achieved significantly better test accuracy than 
randomly chosen labeled points, hinting that
the \acronym\;prior has utility in an active learning setting.

The representations
learned with the \acronym\;
consistently achieve better accuracy,
though  in MNIST, \acronym\;prior
and MAF reach very similar test
accuracy at 100 labels per class.
The advantage of the \acronym\;prior is especially clear
in Omniglot
(\autoref{tab:semisupervised-mnist}
and \autoref{tab:semisupervised-omniglot} contain the exact numbers).
We believe our advantage in this task
comes from ability of the \acronym\;prior
to model discrete structure.
TSNE visualizations
in \autoref{fig:tsne-tmc-normal} and \autoref{fig:tsne}
indicate
clusters are more concentrated and separated
with the \acronym\;prior
than with other priors,
though TSNE visualizations should be
taken with a grain of salt.

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/loracs/mnist/tsne/mnist2-tsne-normal.png}
    \caption{Normal prior}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/loracs/mnist/tsne/mnist2-tsne-tmc.png}
    \caption{\acronym(200) prior}
\end{subfigure}
\caption{TSNE visualizations of the latent space of the MNIST test set with different priors,
color-coded according to class. \acronym\;prior appears to learn a space with more
separated, concentrated clusters.}
\label{fig:tsne-tmc-normal}
\end{figure}

\paragraph{Information retrieval}
We evaluated the meaningfulness of Euclidean distances
in the learned latent space
% Information retrieval can
% help inform whether distance measured
% in the latent space is informative.
% We evaluate this
by measuring precision-recall
when querying the test set.
We take each element of the test set
and sort all other members according to
their $L_2$ distance in the latent space.
From this ranking, we produce
precision-recall curves for each 
of the query and 
plot the average precision-recall
over the entire test set in \autoref{fig:prec-rec}.
We also report the area-under-the-curve (AUC)
measure for each of these curves in \autoref{tab:auc}.

AUC numbers for Omniglot are low across the board
because of the large number of classes
and low number of instances per class.
However,
in both datasets the \acronym\;prior consistently
achieves the highest AUC,
especially with MNIST. 
The \acronym\;prior encourages
tree-distance to correspond to
squared Euclidean distance, as branch
lengths in the tree are 
variances in a Gaussian likelihoods.
We thus suspect distances in a \acronym\;prior
latent space to be more informative
and better for information retrieval.

\paragraph{Held-out log-likelihood}
% Held-out log-likelihood estimates
% the capacity of a generative model.
We estimate held-out log-likelihoods for the four VAEs we
trained with comparable architectures and different priors.
(We exclude DVAE$\sharp$ since its architecture is substantially
different, and the classical autoencoder since it lacks generative
semantics.)
We use 1000 importance-weighted samples \citep{burda2015importance}
to estimate held-out log-likelihood,
and report the results in \autoref{tab:holl}.
We find that, although \acronym\;outperforms the other
priors on downstream tasks, it only achieves
middling likelihood numbers.
This result is consistent with the findings of \citet{chang2009reading} that held-out log-likelihood is not necessarily correlated with interpretability or usefulness for downstream tasks.

\begin{table}
\centering
\input{results/loracs/ir.tex}
\caption{Averaged precision-recall AUC on MNIST/Omniglot test datasets}
\label{tab:auc}
\end{table}

\begin{table}
\centering
\begin{tabular}{r|cc}
\toprule
Prior & MNIST & Omniglot\\ \midrule
%No prior  & N/A & N/A\\
Normal    & -83.789 & -89.722\\
MAF       & \textbf{-80.121} & \textbf{-86.298}\\
Vamp & -83.0135 & -87.604\\
\acronym & -83.401 & -87.105\\
\bottomrule
\end{tabular}
\caption{MNIST/Omniglot test log-likelihoods}
\label{tab:holl}
\end{table}

\section{Discussion}
Learning discrete, hierarchical structure in a latent space
opens a new opportunity:
interactive deep unsupervised learning.
User-provided constraints have been used
in both flat and
hierarchical clustering \citep{wagstaff2000clustering,awasthi2014local}, so an interesting
follow up to this work would be incorporating
constraints into the \acronym\; prior, as
in \citet{vikram2016interactive}, which could potentially
enable user-guided representation learning.

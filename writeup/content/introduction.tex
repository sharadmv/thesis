\chapter{Introduction}

In machine learning problem
settings, we 
are often presented with data
but additionally have
information about how the
data was collected or generated.
Furthermore, we may have
knowledge or access
to domain expertise that 
could inform a learning algorithm.
Traditionally,
one way of incorporating
prior knowledge or domain
expertise into machine
learning algorithms
is by formulating them as probabilistic
graphical models, which
offer flexible avenues for
incorporating assumptions about data.
Take, for example, if we are
tasked with discovering
clusters in a dataset, but have
prior knowledge that
the features of the data are
disentangled or independent,
we can incorporate that
information into the
prior distribution
over cluster components.

This strategy, however,
has major downsides,
namely that the types
of assumptions that
can be incorporated
into probabilistic graphical
models have been historically
limited to generative models in which
Bayesian inference is tractable.
Thus, in scenarios where
the data is high-dimensional
and exhibits complex structure,
we are forced to make
restrictive assumptions
that enable tractable inference,
but limit the capacity
of our model.

On the other hand,
neural networks have emerged
as a dominant force
in machine learning,
thanks to their
ability to scale to large datasets
and their empirical success on complex
data like images and text.
Incorporating prior knowledge
or domain expertise into
deep learning approaches
has often taken the form of
custom neural network layers or
architectures that are tailored
to the task at hand,
and although these strategies
have seen empirical success,
they are often ad-hoc
solutions and do not
leverage the explicit assumptions
and modeling of uncertainty that
probabilistic graphical models offer.
Combining deep learning
methods with probabilistic graphical
models has thus been
recent topic of interest
with many open problems
and challenges.

Much work has gone into investigating
which models are tractable
and how to approximate
intractable ones.
One of the most prominent
methods to emerge in unsupervised learning
is the variational autoencoder \citep[VAE; ][]{Kingma2014, Rezende2014},
which frames autoencoding
as a probabilistic latent variable model.
The VAE models
through a generative process
where latent codes are sampled
from a prior distribution,
and are then passed through a
neural network decoder. 
The encoder is used for inference,
approximating the posterior
distribution of codes given data.

The VAE is a stepping stone
for incorporating
neural networks into
statistical learning,
but in its original form,
has very simple assumptions.
The statistical learning literature,
on the other hand, has
explored a wide variety of
models and structures for data.
In this thesis, we 
motivate and discuss
the use of Bayesian structured priors
and the algorithmic challenges
in incorporating them
with deep generative models.
The combination
of a Bayesian structured
prior with a VAE
results
in a class of models called
Bayesian structured representation learning
models.

In Part I, we introduce the
core ideas and foundations for Bayesian structure
learning and present a contribution
in the space of interactive structure learning.
In Part II, we motivate and discuss
algorithms and models used in Bayesian structured
representation learning.

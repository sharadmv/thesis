\chapter{Introduction}

Neural networks have emerged
as a dominant force
in machine learning,
thanks to their
incredible
ability to scale to large datasets
and their empirical success on high dimensional
data like images and text.
Furthermore, the modularity of deep learning 
approaches has enabled the community
to advance on many fronts in parallel.
Innovations
in one  have influenced


Interest in
in incorporating deep learning
methods into existing
statistical frameworks
has expanded since.
Statistical methods in machine learning
model uncertainty and provide
principled ways of incorporating
discrete structure and 
domain knowledge.
However, merging deep learning
and statistical learning
is challenging, as neural networks
often make Bayesian inference
intractable.
Much work has gone into investigating
what models are tractable
and how to approximate
intractable ones.
One of the most prominent
methods to emerge in unsupervised learning
is the variational autoencoder \citep[VAE; ][]{Kingma2014, Rezende2014},
which frames autoencoding
as a latent variable model.
The VAE models
through a generative process
where latent codes are sampled
from a prior distribution,
and are then passed through a
neural network decoder. 
The encoder is used for inference,
approximating the posterior
distribution of codes given data.

The VAE is a stepping stone
for incorporating
neural networks into
statistical learning,
but in its original form,
uses very simple modeling assumptions.
The statistical learning literature,
on the other hand, has
explored a wide variety of
models and structures for data.
In this thesis, we discuss
Bayesian structure learning methods,
and how to merge them
with the VAE, resulting
in a class of models called
Bayesian structured representation learning
models.
In Part I, we introduce the
core ideas and foundations for Bayesian structure
learning and present a contribution
in the space of interactive structure learning.
In Part II, we motivate and discuss
algorithms and models used in Bayesian structured
representation learning.